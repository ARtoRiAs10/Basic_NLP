{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "od0568Zjt-WC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "ds_train, ds_test = tfds.load('ag_news_subset').values()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 30000\n",
        "batch_size = 128\n",
        "\n",
        "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size, input_shape=(1,))\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    vectorizer,\n",
        "    keras.layers.Embedding(vocab_size,100),\n",
        "    keras.layers.Lambda(lambda x: tf.reduce_mean(x,axis=1)),\n",
        "    keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "mvTstRCyxMAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text(x):\n",
        "    return x['title']+' '+x['description']\n",
        "\n",
        "def tupelize(x):\n",
        "    return (extract_text(x), x['label'])\n",
        "\n",
        "print(\"Training vectorizer\")\n",
        "vectorizer.adapt(ds_train.take(500).map(extract_text))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "model.fit(ds_train.map(tupelize).batch(batch_size), validation_data=ds_test.map(tupelize).batch(batch_size))"
      ],
      "metadata": {
        "id": "3b2nCVY20gGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dealing with variable sequence sizes\n"
      ],
      "metadata": {
        "id": "UJDQlguQ2gqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer('Hello, world!'))\n",
        "print(vectorizer('I am glad to meet you!'))"
      ],
      "metadata": {
        "id": "BJjUzYDz1kjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer(['Hello, world!', 'I am glad to meet you!'])"
      ],
      "metadata": {
        "id": "DTVMiKLZ2vzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[1](vectorizer(['Hello, world!', 'I am glad to meet you!'])).numpy()"
      ],
      "metadata": {
        "id": "gT-I9y1P26hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Semantic embeddings: Word2Vec"
      ],
      "metadata": {
        "id": "xiAdLbeJ4drY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "w2v = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wd6_hg03Fqf",
        "outputId": "998537ad-dafb-43bd-91f8-71c0a5965f42"
      },
      "execution_count": 1,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for w,p in w2v.most_similar('neural'):\n",
        "  print(f\"{w} -> {p}\")"
      ],
      "metadata": {
        "id": "hO5zFBRk4tT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v['play'][:20]"
      ],
      "metadata": {
        "id": "wDFpT2hq4-g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w2v.most_similar(positive=['king', 'woman'], negative=['man'])[0]"
      ],
      "metadata": {
        "id": "zgeAUHBx5glw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#get the vector corresponding to kind-man +woman\n",
        "qvec = w2v['king']-1.7*w2v['man']+1.7*w2v['woman']\n",
        "#find the index of the closest embedding vector\n",
        "d = np.sum((w2v.vectors-qvec)**2, axis=1)\n",
        "min_idx = np.argmin(d)\n",
        "#find the coressponding word\n",
        "w2v.index_to_key[min_idx]"
      ],
      "metadata": {
        "id": "0iRIh5oO5sKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using pretrained embeddings in keras"
      ],
      "metadata": {
        "id": "RGGUvswc_xoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Using tokenizer vocabulary"
      ],
      "metadata": {
        "id": "EP2dIDMz_-WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = len(w2v.get_vector('hello'))\n",
        "print(f'Embedding size: {embed_size}')\n",
        "\n",
        "vocab = vectorizer.get_vocabulary()\n",
        "W = np.zeros((vocab_size, embed_size))\n",
        "print('Populating matrix, this will take some time...',end='')\n",
        "found, not_found =0,0\n",
        "for i,w in enumerate(vocab):\n",
        "  try:\n",
        "    W[i] = w2v.get_vector(w)\n",
        "    found+=1\n",
        "\n",
        "  except:\n",
        "    #W[i] = np.random.normal(0.0,0.3, size=(embed_size,))\n",
        "    not_found+=1\n",
        "\n",
        "print(f\"Done, found {found} words, {not_found} words missing\")"
      ],
      "metadata": {
        "id": "KtglY5XG_Iiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emb = keras.layers.Embedding(vocab_size, embed_size, weights=[W], trainable=False)\n",
        "model = keras.models.Sequential([\n",
        "    vectorizer, emb,\n",
        "    keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=1)),\n",
        "    keras.layers.Dense(4, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "-FSHo2d-Brza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "model.fit(ds_train.map(tupelize).batch(batch_size),\n",
        "          validation_data=ds_test.map(tupelize).batch(batch_size))"
      ],
      "metadata": {
        "id": "ENzvk_4ECOwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Using embedding vocabulary"
      ],
      "metadata": {
        "id": "h4PPl3cqC2zy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(w2v.vocab.keys())\n",
        "vectorizer = keras.layers.experimental.preprocessing.TextVectorization(input_shape=(1,))\n",
        "vectorizer.set_vocabulary(vocab)"
      ],
      "metadata": {
        "id": "wUeEwS7FCsEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential([\n",
        "    vectorizer,\n",
        "    w2v.get_keras_embedding(train,embeddings=False),\n",
        "    keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=1)),\n",
        "    keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
        "model.fit(ds_train.map(tupelize).batch(128), validation_data=ds_test.map(tupelize).batch(128),epochs=5)"
      ],
      "metadata": {
        "id": "j87nUISbE0ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fBzVIQKtFo4L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}